{"meta":{"title":"Nayan Blog","subtitle":"","description":"","author":"NayanTech","url":"https://nayan.co/blog","root":"/blog/"},"pages":[],"posts":[{"title":"Drawing Lines on Images","slug":"Images","date":"2019-12-02T19:27:37.000Z","updated":"2019-12-03T06:28:17.977Z","comments":true,"path":"/2019/12/02/Images/","link":"","permalink":"https://nayan.co/blog/2019/12/02/Images/","excerpt":"","text":"You can draw different types of line on Canvas in Android. You can change it’s color, stroke, effect, etc. You can redo or undo lines , can select and delete them. Here we will see the basics of drawing line on Canvas. Let’s get startedStep -1: Add dependencies12345678910111213// PhotoView implementation &apos;com.github.chrisbanes:PhotoView:2.3.0&apos;//Glide implementation &apos;com.github.bumptech.glide:glide:4.9.0&apos; annotationProcessor &apos;com.github.bumptech.glide:compiler:4.9.0&apos;// Timber for logging implementation &quot;com.jakewharton.timber:timber:$&#123;rootProject.ext.timber_version&#125;&quot;// Coroutines implementation &quot;org.jetbrains.kotlinx:kotlinx-coroutines-core:$&#123;rootProject.ext.coroutines_version&#125;&quot; implementation &quot;org.jetbrains.kotlinx:kotlinx-coroutines-android:$&#123;rootProject.ext.coroutines_version&#125;&quot; implementation &quot;androidx.lifecycle:lifecycle-viewmodel-ktx:$&#123;rootProject.ext.lifecycle_extensions_version&#125;&quot; implementation &quot;androidx.lifecycle:lifecycle-runtime-ktx:$&#123;rootProject.ext.lifecycle_extensions_version&#125;&quot; implementation &quot;androidx.lifecycle:lifecycle-livedata-ktx:$&#123;rootProject.ext.lifecycle_extensions_version&#125;&quot; Step -2: Create bitmap of Image and canvas to drawThe first step is to create a bitmap of Image on which you want to draw lines. We are using Glide to download image and to get bitmap from it. 1234567private suspend fun getOriginalBitmapFromUrl(): Bitmap = withContext(Dispatchers.IO) &#123; Glide.with(this@MainActivity) .asBitmap() .load(Constants.IMAGE_URL) .submit().get() &#125; Step -3: Create event listenerCreate an interface to handle onTouch events of photoviewattacher. 123fun touchPoints(x: Float, y: Float)fun closeStroke()fun discardStroke() Step -4: Create your overlay view attacherNow we create overlayview attacher, where we define onTouch events. 123456789101112131415161718192021222324MotionEvent.ACTION_UP -&gt; &#123; if (multiTouch) &#123; onPencilDrawListener.discardStroke() multiTouch = false &#125; else &#123; onPencilDrawListener.closeStroke() &#125;&#125;MotionEvent.ACTION_DOWN -&gt; &#123; if (!multiTouch &amp;&amp; isSelectModeEnabled) &#123; sendEventToListener(ev) &#125; else &#123; super.onTouch(v, ev) &#125;&#125;MotionEvent.ACTION_MOVE -&gt; &#123; if (!multiTouch) &#123; sendEventToListener(ev) &#125; else &#123; super.onTouch(v, ev) &#125;&#125; Step -5: Create your overlay photo viewCreate your own imageview by extending it from photoview and added some properties a. For selecting and deleting lines:If the angle at touch point is an obtuse angle and area of triangle formed is under the threshold limit, then the current line is found to be selected To calculate area:1abs((touchX * (y1 — y2) + x1 * (y2 — touchY) + x2 * (touchY — y1)) / 2) where (x1,y1) and (x2,y2) are endpoints of selected line. To calculate angle:12345678910111213141516171819202122private fun calculateAngle( tapPoint: List&lt;Float&gt;, linePoint1: List&lt;Float&gt;, linePoint2: List&lt;Float&gt;): Float &#123; // Square of lengths be a2, b2, c2 val a2 = lengthSquare(linePoint1, linePoint2) val b2 = lengthSquare(tapPoint, linePoint2) val c2 = lengthSquare(tapPoint, linePoint1) // length of sides be b, c val b = sqrt(b2) val c = sqrt(c2) // From Cosine law var alpha = acos((b2 + c2 - a2) / (2f * b * c)) // Converting to degree alpha = (alpha * 180 / PI).toFloat() return alpha&#125; b. For redo operation:If last undo operation list is not empty, add the first line points from last undo operation list to overlay data list and remove it from there. 12345678910111213141516171819fun redo() &#123; if (lastUndoOperation != null) &#123; if (lastUndoOperation?.isDeleteOperation == true) &#123; val ids = lastUndoOperation?.overlayDataList?.map &#123; overlayData -&gt; overlayData.id &#125; if (ids != null) &#123; for (data in overlayDataList) &#123; if (data.id in ids) &#123; data.isDeleted = true &#125; &#125; &#125; &#125; else &#123; lastUndoOperation?.overlayDataList?.first()?.let &#123; overlayDataList.add(it) &#125; &#125; lastUndoOperation?.let &#123; overlayDataOperations.add(it) &#125; lastUndoOperation = null invalidate() &#125;&#125; c. For undo operation:If there is any point present in overlay data list then delete it from there and added it in last undo operation list. 12345678910111213141516171819202122fun undo() &#123; if (overlayDataOperations.size &gt; 0) &#123; val index = overlayDataOperations.lastIndex lastUndoOperation = overlayDataOperations[index] overlayDataOperations.removeAt(index) if (lastUndoOperation?.isDeleteOperation == true) &#123; val ids = lastUndoOperation?.overlayDataList?.map &#123; overlayData -&gt; overlayData.id &#125; if (ids != null) &#123; for (data in overlayDataList) &#123; if (data.id in ids) &#123; data.isDeleted = false &#125; &#125; &#125; &#125; else &#123; val id = lastUndoOperation?.overlayDataList?.first()?.id overlayDataList.removeAt(overlayDataList.indexOfFirst &#123; overlayData -&gt; overlayData.id == id &#125;) &#125; invalidate() &#125;&#125; For more sample code , see the Draw-Line. And we’re done!Screenshots:","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/blog/categories/Android/"}],"tags":[],"author":"Diwakar Singh"},{"title":"Android Testing Strategy","slug":"Android-Testing-Strategy","date":"2019-11-29T17:07:05.000Z","updated":"2019-12-03T06:28:17.969Z","comments":true,"path":"/2019/11/29/Android-Testing-Strategy/","link":"","permalink":"https://nayan.co/blog/2019/11/29/Android-Testing-Strategy/","excerpt":"","text":"Testing android application is quite hard. There is no set guidelines for us to follow. When ever I started thinking of testing my application I always get confused where to start, should I write unit tests or instrumentation tests and should I start with integration and End to end tests. There is also a lot of confusion on frameworks available for android testing.This article has been written in a sense to address this confusion and show us as developers what kind of testing is most preferred and what frameworks are available. Kinds of TestsUnit TestThis is often referred to as local tests and doesn’t require a device or emulator for running them. These can be classified broadly into categories Local/ Pure Unit tests - tests which can run on JVM, mainly written for testing business logic. On android JUnit, Mockito, the mockable Android JARs give us a nice combination of tools to run fast unit-style tests in the local JVM. Android Unit Tests - tests which requires the Android system (ART) and for this we need to replace android dependencies using RoboelectricGuidelines If you have dependencies on the Android framework, particularly those that create complex interactions with the framework, it’s better to include framework dependencies using Robolectric. If your tests have minimal dependencies on the Android framework, or if the tests depend only on your own objects, it’s fine to include mock dependencies using a mocking framework like Mockito and PowerMock. Reference Url : https://developer.android.com/training/testing/unit-testing/local-unit-tests Instrumentation TestsThese tests requires device or an emulator for running. This is mostly used for UI testing but it can be used to test none UI logic as well. This is useful when you need to test code that has a dependency on context. UI tests can be an essential component of any testing strategy since they can uncover issues related to UI, hardware, firmware, and backwards compatibility Reference Url: https://developer.android.com/training/testing/unit-testing/instrumented-unit-tests Testing FrameworksAndroid X Test Framework It is testing framework and APIs provided by android team for writing unit tests. RoboelectricIt is a framework that brings fast and reliable unit tests to android. Runs inside JVM or your workstation in seconds. This is usually used to Integration testing. Integration tests validate how your code interacts with other parts of the system but without the added complexity of a UI framework. app/src/test/java - for any unit test which can run on the JVM Question : How does it work?Answer : Unlike traditional emulators-based androids tests, it tests run inside a sandbox which allows the android environment to be precisely configured to the desired conditions for each test. It lets you run your tests on your workstation, or on your continuous integration environment in a regular JVM, without an emulator. It handles inflation of views, resource loading, and lots of other stuff that’s implemented in native C code on Android devices. This allows tests to do most things you could do on a real device. It’s easy to provide our own implementation for specific SDK methods too, so you could simulate error conditions or real-world sensor behaviour. It allows a test style that is closer to black box testing, making the tests more effective for refactoring and allowing the tests to focus on the behaviour of the application instead of the implementation of Android Question : Why should be prefer this?Answer : In order for this to run tests it needs regular JVM, Because of this, the dexing, packaging, and installing-on-the emulator steps aren’t necessary, reducing test cycles from minutes to seconds so you can iterate quickly and refactor your code with confidence. Robolectric executes your code against real (not mock) Android JARs in the local JVM. Espresso Use it to write concise, beautiful, and reliable Android UI tests. These tests are called Instrumentation tests and unlike unit tests takes more time to run them. app/src/androidTest/java - for any instrumentation test which should run on an Android Question : How does it work?Answer : it requires an emulator or a real device to run tests. At the time of execution along with the main application, A testing application is also installed in the device which controls main application automatically. UI AutomatorIt allows us to write cross application functional tests ( End to End) . Example, Sharing messages via Text intent or sending email via locally installed email clients. Monkey Runner CL Monkey is a command line tool which sends pseudo random events to your device. You can restrict Monkey to run only for a certain package and therefore instruct Monkey to test only your application. it can be used for Stress testing for android. Reference Url : https://developer.android.com/studio/test/monkey Recommandations Creating test groups - @SmallTest. @MediumTest and @LargeTest annotation allows us to classify tests. Allows you to run, for example, only short running tests for development cycle. You may run your long running tests on a continuous integration server.This can be easily configured this via InstrumentationTestRunner in user build.gradle (app) We can use a three tiered approach Pure Unit tests : These can be written for our business logic which are completely android independent of API and can run on JVM. These can be written using Junit Framework. Roboelectric Unit tests: where code has only small dependencies on android APIs and can be easily mocked with Roborelectric. Android Instrumentation tests : where code heavily interact with device hardware, sensors and android APIs. These tests will usually take most time to run.","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/blog/categories/Android/"}],"tags":[{"name":"Instrumentation Test","slug":"Instrumentation-Test","permalink":"https://nayan.co/blog/tags/Instrumentation-Test/"},{"name":"Unit Test","slug":"Unit-Test","permalink":"https://nayan.co/blog/tags/Unit-Test/"},{"name":"Android Testing","slug":"Android-Testing","permalink":"https://nayan.co/blog/tags/Android-Testing/"}],"author":"Puneet Kashyap"},{"title":"Tracking Deep Learning experiments using Keras,MlFlow and MongoDB","slug":"Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb","date":"2019-11-29T12:06:56.000Z","updated":"2019-12-03T06:28:17.981Z","comments":true,"path":"/2019/11/29/Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb/","link":"","permalink":"https://nayan.co/blog/2019/11/29/Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb/","excerpt":"","text":"It is late 2019 and Deep Learning is not a buzzword anymore. It is significantly used in the technology industry to attain feats of wonders which traditional machine learning and logic based techniques would take a longer time to achieve. The main ingredient in Deep Learning are Neural Networks, which are computation units called neurons, connected in a specific fashion to perform the task of learning and understanding data. When these networks become extremely deep and sophisticated, they are referred to as Deep Neural Networks and thus Deep Learning is performed. Neural Networks are so called because they are speculated to be imitating the human brain in some manner. Though it is not entirely true, but the learning mechanism is mostly similar in nature. A human brain learns about an object or concept when it visually experiences it for a longer amount of time. Similar to that, a neural network learns about objects and what they actually represent when it is fed with a large amount of data. For example, let us consider the LeNet architecture . It is a small two layered CNN (Convolution Neural Network). Convolution Neural Networks are a special kind of neural network where the mathematical computation being done at every layer are convolution operations. If enough images of a certain kind are fed to the LeNet architecture, it starts to understand and classify those images better. That was a simple introduction to what neural networks are and how they behave. In this article we will be mostly looking into three main frameworks which can ease out the developer experience of building these neural networks and tracking there performance efficiently. Nowadays, neural networks are heavily used for classifying objects, predicting data and other similar tasks by many companies out there. When it comes it to training neural networks and keeping track of their performance, the experience is not too subtle. When building a neural network, a developer would be trying out multiple datasets and experimenting with different hyperparameters. It is essential to keep a track a of these parameters and how they affect the output of the neural networks. Also debugging neural networks is an extremely cumbersome task. The output performance of different neural networks may vary due to different reasons. Some of the possible causes maybe inadequate data pre-processing, incorrect optimizer, a learning rate which is too low or too high. The number of variables which affect the performance of a neural network are quite a few. Hence it is essential that every parameter is properly tracked and maintained. Some of the available options present out there include the infamous *Tensorboard, Bokeh *to name a few. In this project we will be using MlFlow ,an open source platform to manage the entire deep learning development cycle. MlFlow allows developers to log and track the outputs of every experiment performed with great precision. We will be looking into MlFlow’s components with more detail in the subsequent sections. The framework we would be using for writing our neural networks and training them is Keras. We will be using the FashionMNIST dataset. It contains a total of 70000 gray scale images (training:test = 60000:10000) , each scaled at 28x28 associated with one from 10 classes. (Fig 1) The folder structure of our project looks as shown in Fig 2 below. The data folder contains our fashion mnist dataset files which will be used for training the model. The db folder contains the python driver code to perform operations on MongoDb collections. MongoDB is an extremely easy to use NoSql database which has been built keeping in developer satisfaction. It is easily integrable with modern day applications and has a large developer community contributing to it’s extensions regularly. The model folder contains piece of code with the neural network model definition. The mlruns folder is created automatically once *mlflow *is invoked in the main code. The aim of the project is to track multiple deep learning experiments and check how the outputs are affected when parameters are changed and data is changed. Since we have only one dataset, we will split it into equal parts in order to simulate a multiple dataset scenario. Let’s start off with the create_dataset script, which is used to split the fashion mnist into equal parts and store them inside the data folder with proper serial number. In Fig 3 shown below, we import fashion mnist from *keras.datasets *and perform the necessary normalization step Next, we write the methods to split the dataset into equal parts and save them with proper incremental serial numbers inside the data folder, inside the root project directory (Fig 4) equal parts for training* After this we go ahead and write the necessary driver code to manage our newly created dataset using MongoDb. Some might say that using MongoDb for such a small project might be an overkill, but personally I find MongoDb to be an excellent tool for managing data with flexibility. Given NoSql’s schema-less nature, managing collections and documents is a breeze. The ease with which any document can be edited in MongoDB is superb. The best part is, whenever a collection is queried , the result returned is a *json *making it extremely easy to be parsed using any programming language. Aggregation queries in mongo are also very simple and allows users to cross reference collections in a swift manner. In order to connect our python scripts with MongoDb we will be using pymongo which can be easily installed using the pip install pymongo. To install MongoDb, follow this tutorial ***https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/*** Once MongoDb is installed and tested to be running properly on your system, create a new database called fashion_mnist. Inside the database create a new collection named dataset as shown in Fig 5 below. A great GUI to interact with MongoDb is robo3t. It’s free and easy to use. It can be downloaded from the following link ***https://robomongo.org/download.***Since our DB is setup and datasets are created, we can progress with the task of inserting necessary information into the dataset collection In Fig 6 shown above, we are importing MongoClient from the pymongo library which will essentially be used to connect to our mongoDB database. Fig 7 below describes mongoQueue class which has been written in order to interact with our dataset collection. In line 18 and 19, the collection name is initialized, which is used in all the member functions. The Enqueue* method in line 6 is used to insert dataset information into the dataset collection. The *Dequeue method in line 10 fetches the first dataset which has a status field of ‘Not** *Processed’.* The setAsProcessing* and setAsProcessed *methods are used to set the status field of respective dataset documents in the collection. Fig 8: Methods to insert data into MongoDB We use the insert_into_db method shown in Fig 8, line 1, to insert information about our newly created datasets into our mongoDb dataset collection. In line 23 of the main function, we iterate over the dataset folder and call *insert_into_db *to insert the necessary information for that dataset into the collection. Once every dataset is successfully inserted into the collection, the fields appear as shown in Fig 9 below. We can now define our model for training our deep learning network. Inside *model/model.py we import all necessary **keras*** packages to build our CNN network (shown in Fig 10a) Fig 10b shows the model architecture. It is a simple two layer CNN, with two MaxPool layers and RelU activation in between. Two Dense layers are also added with 32 and 10 neurons respectively. I have also added a Dropout of 0.5 before the last Dense layer. Now, in our train.py script we import all the necessary modules needed from the keras library to get on with our training. Along with all the keras libraries we import *mlflow *as well (Fig 11) All the hyperparameters which will be used for training is stored in a config file named as train_config.json. This file is read (Fig 12a) and used for defining training parameters In Fig 12b, we have defined out training function , which takes arguments *trainX (our training set) ,*trainY** (*training set labels) and the **model** *(CNN model) From line 38 (In Fig 13), the main function starts where we define our *MongoQueue object using the **MongoQueue** class defined inside script mentioned before, **db_ops.py . *As it can be seen in line 41, *mq*** is our object. In line 42 (Fig 13) , a new CNN model is created by calling the *model function which accepts the optimizer type as input. In this experiment we would be using the ‘*SGD’ (Stochastic Gradient Descent**) to train the network . Everytime we invoke mlflow in our training code for logging, it is known as an mlflow run. MlFlow provides us with an API for starting and managing MlFlow runs. For example, Fig 14a and 14b In our code we start the mlflow run using the python context manager as shown in Fig 14b. At line 46 in Fig 13, we define our our mlflow run with the run name as ‘fashion mnist’.* All data and metrics will be logged under this run name on the **mlflow*** dashboard. From line 47 we start a while loop, which continuously invokes the *dequeue function from the *MongoQeueue** class. What this does is fetches every row corresponding to a particular dataset from the dataset collection which has a status field = *Not Processed (Fig 9). As soon as this dataset is fetched, *setAsProcessing* function is called in line 51 which sets the status of that dataset to *Processing *in *MongoDb. This enables us to understand which dataset is currently being trained by our system. This is particularly helpful is large systems where there are multiple datasets and many training instances running in parallel. In lines 54 and 55, the datasets are loaded from the *data folder corresponding to the **dataset_id** *fetched from the db. Lines 57 and 58 loads the test sets and the training is started at line 59 by calling the *train *function. We then use the trained model to predict our scores as shown in line 60 in Fig 15. The training output looks as shown below (Fig 16a and Fig 16b) As shown in Fig 16b, the training happens for an epoch and the evaluation metrics for the test dataset gets logged. All outputs of the evaluation done using our trained model can be logged using the MlFlow tracking api (as shown in Fig 17). The tracking API comes with functions such as log_param and log_metric which enables us to log every hyperparameter and output values into mlflow. The best feature about mlflow is the dashboard it provides. It has a very intuitive UI and can be used efficiently for tracking our experiments. The dashboard can be started easily by simply hitting *mlflow ui *in your terminal as shown in Fig 18 below. To access the dashboard, just type ***http://localhost:5000** *in your browser and hit enter. Fig 19 shows how the dashboard looks like. Each MlFlow run is logged using a run ID and a run name. The Parameters and the Metrics column log display the parameters and the metrics which were logged while we were training our model Further clicking on a particular run, takes us to another page where we can display all information about our run (Fig 20) MlFlow provides us with this amazing feature to generate plots for our results. As you can see in Fig 21a, the test accuracy change can be visualized across different training datasets and time. We can also choose to display other metrics such as the eval loss, as shown in Fig 21b. The smoothness of the curve can also be controlled using the slider. We can also log important files or scripts in our project to MlFlow using the mlflow.log_artifact command . Fig 22a shows how to use it in your training script and Fig 22b shows how it is displayed on the mlflow dashboard. MlFlow also allows users to compare two runs simultaneously and generate plots for it. Just tick the check-boxes against the runs you want to compare and press on the blue Compare button (Fig 23) Once you click on compare, another page pops up where all metrics and parameters of two different runs can be viewed and studied in parallel (Fig 24a) The user can also choose to display metrics such as accuracy and loss in parallel charts as shown in Fig 24b. Users can add an MlFlow Project file (a text file in YAML syntax) to their MlFlow project allowing them to package their code better and run and recreate results from any machine. The MlFlow Project file for our fashion_mnsit project looks as shown below in Fig 25a We can also specify a conda environment for our MlFlow project and specify a conda.yaml file (Fig 25b). Hence, with this we conclude our project. Developers face several on-demand requirements for monitoring metrics during training a neural network. These metrics can be extremely critical for predicting the output of their neural networks and are also critical in understanding how to modify a neural network for better performance. Traditionally when starting off with deep learning experiments, many developers are unaware of proper tools to help them. I hope this was piece of writing was helpful in understanding how deep learning experiments can be conducted in a proper manner during production when large numbers of datasets are needed to be managed and multiple training and evaluation instances are required to be monitored. MlFlow also has multiple other features which is beyond the scope of this tutorial and can be covered later. For any information on how to use MlFlow one can head to the ***https://mlflow.org/***","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/blog/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/blog/tags/Deep-Learning/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/blog/tags/Machine-Learning/"},{"name":"Mlflow","slug":"Mlflow","permalink":"https://nayan.co/blog/tags/Mlflow/"},{"name":"Keras","slug":"Keras","permalink":"https://nayan.co/blog/tags/Keras/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://nayan.co/blog/tags/MongoDB/"},{"name":"Tracking","slug":"Tracking","permalink":"https://nayan.co/blog/tags/Tracking/"}],"author":"Abhishek Bose"},{"title":"License plate recognition using Attention based OCR","slug":"License-plate-recognition-using-Attention-based-OCR","date":"2019-11-28T12:06:56.000Z","updated":"2019-12-03T06:28:17.977Z","comments":true,"path":"/2019/11/28/License-plate-recognition-using-Attention-based-OCR/","link":"","permalink":"https://nayan.co/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/","excerpt":"","text":"IntroductionIf you clicked on this post title then it is certain that you are working on some kind of License plate recognition task and working on a specific kind of License Plate, if that’s the case you have landed in right post. In this post i will explain how to train Attention based OCR (AOCR) model for a specific License Plate. I will first share some brief information of AOCR model followed by steps which will help you train the model and after that we will use the trained model to test its performance. Attention OCR Model architectureFirst of all the source code of this model is available on this Tensorflow Github repository. I will suggest you to try this repository if you want/can modify code. Figure 1. AOCR model architecture Source: https://arxiv.org/pdf/1609.04938v2.pdf Now the model structure, it has 7 Convolutional layer and 3 bi-directional Long short-term memory (LSTM) layers followed by a Seq2Seq model which translates image features to characters(act as a decoder). Convolutional layers can be seen in the below image. Figure 2. CNN architecture How to train this model?First of all use this pip command to install aocr on your system. pip install aocrNow you need dataset of License Plates images with its annotation. Annotation file should have file path and its label in a text file. datasets/image1.jpg label1 datasets/image2.jpg label2After the dataset is ready along with annotation file you have to run this command to create tfrecords file for training of AOCR model. Separate some images for testing and create separate tfrecords file using test annotation file which contains paths of test images and corresponding labels. aocr dataset /path/to/training_annotation.txt path/to/training.tfrecords aocr dataset /path/to/testing_annotation.txt path/to/testing.tfrecordsThe above command need annotation file path as input and creates tfrecords file on the given path i.e. last argument of above command. Now just run the below command to start training procedure. By default maximum width is set to 160 and maximum height is set to 60. If your images has width or height more than the default maximum then model will not use those images for training. Either you resize all your images or you can set maximum width and height just make sure all the images are below those values. Default checkpoint directory is ‘./checkpoints’ and it will create this where you will execute below command(You can set this too). Just make sure when you test you are giving correct checkpoint path, width and height. Maximum prediction length is 8 by default and again you can change it according to your License plates. Default epoch is 1000 change it according to quantity of your dataset if it is small run it for default value otherwise you can set it to 500. aocr train /path/to/training.tfrecords --max-width 200 --max-height 100 --model-dir /path/to/checkpoint --max-prediction 6 --num-epoch 500Test the modelOnce the training procedure is finished use this command to test the model. Just make sure checkpoint directory is created when the training starts. aocr test /path/to/testing.tfrecords --visualize --max-width 200 --max-height 100 --model-dir /path/to/checkpoint --max-prediction 6 --output-dir ./resultsNow you can see all the prediction inside result folder. For each file there will be one folder which will contain the GIF which will have attention mapped on the images and a text file which will have prediction in the first line and label in the second line. Prediction is placed on the folder name too by default. Figure 3. Result folder directory Figure 4. Text file which contains prediction and label Gif 1. GIF with attention map ConclusionIf you have successfully trained and test the model then you can skip this part. If you have all the training images in one folder and their labels are in the filename itself then you can run this simple script to train your model. Note:In case of same label for different images filename will be label_1.extension, label_2.extension etc. Execute this script using this command “python3 Train_AOCR.py -d /home/some/path/” 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import cv2import osimport shutilimport sysfrom pathlib import Pathimport optparse#python3 Train_AOCR.py -d /DIR_PATH/# Give checkpoint path, steps per checkpoints and number of epoch in line 61#give images max width and height heredim = (210, 70)parser = optparse.OptionParser()parser.add_option(&apos;-d&apos;, &apos;--dir_path&apos;, action=&quot;store&quot;, dest=&quot;dirpath&quot;, help=&quot;Enter test image directory&quot;, default=&quot;Empty&quot;)parser.add_option(&apos;-i&apos;, &apos;--image&apos;, action=&quot;store&quot;, dest=&quot;image&quot;, help=&quot;Input image&quot;, default=&quot;Empty&quot;)options, args = parser.parse_args()if os.path.exists(&quot;/home/username/path/annotations-training.txt&quot;): os.remove(&quot;/home/username/path/annotations-training.txt&quot;)if os.path.exists(&quot;/home/username/path/train.tfrecords&quot;): os.remove(&quot;/home/username/path/train.tfrecords&quot;)f_veh = open(&apos;/home/username/path/annotations-training.txt&apos;, &apos;w+&apos;)if options.dirpath != &apos;Empty&apos;: for filename in os.listdir(options.dirpath): name, ext = os.path.splitext(filename) name = name.split(&apos;_&apos;) img = cv2.imread(options.dirpath+filename) img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA) cv2.imwrite(options.dirpath+filename,img) #os.rename(options.dirpath+filename,options.dirpath+temp[0]+ext) if ext in [&apos;.png&apos;, &apos;.jpg&apos;,&apos;.jpeg&apos;]: f_veh.write(options.dirpath+filename+ &apos; &apos;+name[0]+ &apos;\\n&apos;) comm = &apos;aocr dataset /home/username/path/annotations-training.txt /home/username/path/train.tfrecords&apos;comm1 = &apos;aocr train /home/username/path/train.tfrecords --model-dir /home/username/path/checkpoints --max-height 70 --max-width 210 --max-prediction 6 --num-epoch 1000&apos; os.system(comm)os.system(comm1) Now you can test the model on a test set using the above code only same format goes as for the training set and its label. You just need to run below code using this command “python3 Run_AOCR.py -d /home/some/test_set_path/” 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import cv2import osimport shutilimport sysfrom pathlib import Pathimport optparse#python3 Run_AOCR.py -d /DIR_PATH/# Give checkpoint path, steps per checkpoints and number of epoch in line 61#give images max width and height heredim = (210, 70)parser = optparse.OptionParser()parser.add_option(&apos;-d&apos;, &apos;--dir_path&apos;, action=&quot;store&quot;, dest=&quot;dirpath&quot;, help=&quot;Enter test image directory&quot;, default=&quot;Empty&quot;)parser.add_option(&apos;-i&apos;, &apos;--image&apos;, action=&quot;store&quot;, dest=&quot;image&quot;, help=&quot;Input image&quot;, default=&quot;Empty&quot;)options, args = parser.parse_args()p = Path(&quot;/home/username/path/results&quot;)if p.is_dir(): shutil.rmtree(&apos;/home/username/path/results&apos;) if os.path.exists(&quot;/home/username/path/annotations-testing.txt&quot;): os.remove(&quot;/home/username/path/annotations-testing.txt&quot;)if os.path.exists(&quot;/home/username/path/test.tfrecords&quot;): os.remove(&quot;/home/username/path/test.tfrecords&quot;)f_veh = open(&apos;/home/username/path/annotations-testing.txt&apos;, &apos;w+&apos;)if options.dirpath != &apos;Empty&apos;: for filename in os.listdir(options.dirpath): name, ext = os.path.splitext(filename) name = name.split(&apos;_&apos;) img = cv2.imread(options.dirpath+filename) img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA) cv2.imwrite(options.dirpath+filename,img) #os.rename(options.dirpath+filename,options.dirpath+temp[0]+ext) if ext in [&apos;.png&apos;, &apos;.jpg&apos;,&apos;.jpeg&apos;]: f_veh.write(options.dirpath+filename+ &apos; &apos;+name[0]+ &apos;\\n&apos;) comm = &apos;aocr dataset /home/username/path/annotations-testing.txt /home/username/path/test.tfrecords&apos;comm1 = &apos;aocr test --visualize /home/username/path/test.tfrecords --model-dir /home/username/path/checkpoints --max-height 70 --max-width 210 --max-prediction 6 --output-dir ./results&apos; os.system(comm)os.system(comm1) After running this script you can find all the prediction in the output directory.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/blog/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/blog/tags/Deep-Learning/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/blog/tags/Machine-Learning/"},{"name":"OCR","slug":"OCR","permalink":"https://nayan.co/blog/tags/OCR/"},{"name":"AOCR","slug":"AOCR","permalink":"https://nayan.co/blog/tags/AOCR/"},{"name":"License Plates","slug":"License-Plates","permalink":"https://nayan.co/blog/tags/License-Plates/"},{"name":"License Plates Recognition","slug":"License-Plates-Recognition","permalink":"https://nayan.co/blog/tags/License-Plates-Recognition/"}],"author":"Piyush Jain"},{"title":"Sharing modules across Android apps","slug":"sharing-modules-across-android-apps","date":"2019-11-27T13:00:00.000Z","updated":"2019-12-03T06:28:17.997Z","comments":true,"path":"/2019/11/27/sharing-modules-across-android-apps/","link":"","permalink":"https://nayan.co/blog/2019/11/27/sharing-modules-across-android-apps/","excerpt":"","text":"Sharing modules across Android appsWhile most android apps are created with a single default app module, in the last few years people have startedmoving to a multi module structure for their Android apps. Having multiple smaller modules have a few distinctadvantages Build times are noticeably faster Your code is decoupled with clear dependencies Better distribution of ownership across different parts of the app Allows modules to be reused across apps Example modulesA possible strategy is to have one module per feature. app: This is the main module which will be the entry point into your app. It acts mainly as a coordinator betweenother modules core: This will contain the model definitions that are core to your app and will be required across modules networking: This provides the networking code for the other modules login: Login/Signup logic goes here dashboard: User dashboard will be here There are many posts on the advantages and strategies for modularizing your Android apps. For this post, we willfocus on the strategy to reuse modules across apps. Reusing modules across appsWe have multiple apps in our company that share the core and login logic. So we decided to share these modules amongthe two applications. One obvious way to share Android Library modules would be to share the generated .aar files and add them asdependencies to the different apps. While this is simpler, the main applications and the library modules will bedifferent Android Studio projects. If any change needs to be done in the library, the .aar will have to be regeneratedand manually updated. There has to be a better way. The solution we decided to use for sharing modules is git submodules. Though it had a small overhead in bringingthe entire team up to speed with submodules, it has worked exceptionally well for us. In the above example, we have two git submodules, core and login. And the submodules will be added as dependencies just as any Android module, Creating a new submodulesTo create a new submodule, we follow the following process Create an empty repo on Github and initialize with a README Add a new submodule to our app git submodule add git@github.com:username/core.git Create a new Android Library module in the new directory Commit the changes in the library module and push Commit the changes in the main repo and push Next time we need to use this submodule in another app, we only need to Add a new submodule to our app git submodule add git@github.com:username/core.git Add the newly added module to settings.gradle and a dependency in build.gradle Committing changes to a submoduleEvery time we make some changes to a submodule, we just need to make sure that we commit and push those changesbefore committing the changes in the main repo. Submodule: If you are on a detached head, git checkout -b new-branch git add . &amp;&amp; git commit -am &quot;commit message&quot; git push origin new-branch Main repo: git add . &amp;&amp; git commit -am &quot;commit message&quot; git push Fetching remote changesEvery time we do a git pull, we just need to remember to update the submodules as well git submodule update and that’s it. We have the latest version of the submodule locally! Hope this works for you. Happy coding!","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/blog/categories/Android/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/blog/tags/android/"},{"name":"git submodules","slug":"git-submodules","permalink":"https://nayan.co/blog/tags/git-submodules/"}],"author":"Anuj Middha"},{"title":"Text detection in number plates","slug":"Text-detection-in-number-plates","date":"2019-11-26T15:21:29.000Z","updated":"2019-12-03T06:28:17.977Z","comments":true,"path":"/2019/11/26/Text-detection-in-number-plates/","link":"","permalink":"https://nayan.co/blog/2019/11/26/Text-detection-in-number-plates/","excerpt":"","text":"Text detection in number platesOne of the vital modules in the optical character recognition(OCR) pipeline is text detectionand segmentation which is also called text localization. In this post, we will apply variedpreprocessing techniques to the input image and find out how to localize text in theenhanced image, so that we can feed the segments to our text recognition network. Image PreprocessingSometimes images can be distorted, noisy and other problems that can scale back the OCRaccuracy. To make a better OCR pipeline, we need to do some image preprocessing. Grayscale the image: Generally you will get an image which is having 3channels(color images), we need to convert this image into a grayscale form whichcontains only one channel. We can also process images with three channels but itonly increases the complexity of the model and increases the processing time.OpenCV provides a built-in function that can do it for you. 12import cv2grayscale_image = cv2.cvtColor(image, cv2.COLOR_BRG2GRAY) Or you can convert the image to grayscale while reading the image. 12#opencv reads image in BGR formatgraysacle_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) Noise reduction: Images come with various types of noises. OpenCV provides a lot ofnoise reduction function. I am using the Non-local Means Denoising algorithm. 1denoised_image = cv2.fastNlMeansDenoising(grayscale_img, None, 10, 7, 21) Contrast adjustment: Sometimes we have low contrast images. This makes it difficultto separate text from the image background. We need high contrast text images forthe localization process. We can increase image contrast using Contrast LimitedAdaptive Histogram Equalization (CLAHE) among many other contrast enhancementmethods provided by skimage. 12from skimage import exposurecontrast_enhanced_image = exposure.equalize_adapthist(denoised, clip_limit=0.03) So now we are done with image preprocessing let us move on to the second part, textlocalization. Text LocalizationIn this part, we will see how to detect a large number of text region candidates andprogressively removes those less likely to contain text. Using the MSER feature descriptor tofind text candidates in the image. It works well for text because the consistent color and highcontrast of text lead to stable intensity profiles. 123#constructor for MSER detectormser = cv2.MSER_create()regions, mser_bboxes = mser.detectRegions(contrast_enhance_image) Along with the text MSER picked up many other stable regions that are not text. Now, thegeometric properties of text can be used to filter out non-text regions using simplethresholds. Before moving on with the filtering process, let’s write some functions to display the results ina comprehensible manner. 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as plt#display imagesdef pltShow(*images): #count number of images to show count = len(images) #three images per columnn Row = np.ceil(count / 3.) for i in range(count): plt.subplot(nRow, 3, i+1) if len(images[i][0], cmap=’gray’) plt.imshow(images[i][0], cmap=’gray’) else: plt.imshow(images[i][0]) #remove x-y axis from subplots plt.xticks([]) plt.yticks([]) plt.title(images[i][1]) plt.show()#color each MSER region in imagedef colorRegion(image_like_arr, region): image_like_arr[region[:, 1], region[:, 0], 0] = np.random.randint(low=100, high=256) image_like_arr[region[:, 1], region[:, 0], 1] = np.random.randint(low=100, high=256) image_like_arr[region[:, 1], region[:, 0], 2] = np.random.randint(low=100, high=256) return image The geometric properties we are going to use to discriminate between text and non-textregion are: Region area Region perimeter Aspect ratio Occupancy Compactness We will apply simple thresholds over these parameters to eliminate non-text regions. Firstlet’s write method to compute these parameters. 12345678910111213141516171819202122232425262728293031#values for the parametersAREA_LIM = 2.0e-4PERIMETER_LIM = 1e-4ASPECT_RATIO_LIM = 5.0OCCUPATION_LIM = (0.23, 0.90)COMPACTNESS_LIM = (3e-3, 1e-1)def getRegionShape(self, region): return (max(region[:, 1]) - min(region[:, 1]), max(region[:, 0]) - min(region[:, 0])) #compute areadef getRegionArea(region): return len(list(region))#compute perimeterdef getRegionPerimeter(image, region): #get top-left coordinate, width and height of the box enclosing the region x, y, w, h = cv2.boundingRect(region) return len(np.where(image[y:y+h, x:x+w] != 0)[0])) #compute aspect ratiodef getAspectRatio(region): return (1.0 * max(getRegionShape(region))) / (min(getRegionShape(region)) + 1e-4)#compute area occupied by the region area in the shapedef getOccupyRate(region): return (1.0 * getRegionArea(region)) / (getRegionShape(region)[0] * \\getRegionShape(region)[1] + 1.0e-10) #compute compactness of the regiondef getCompactness(region): return (1.0 * getRegionArea(region)) / (1.0 * getRegionPerimeter(region) ** 2) Now apply these methods to filter out text regions as follows: 123456789101112131415161718192021222324252627282930#total number of MSER regionsn1 = len(regions)bboxes=[]for i, region in enumerate(regions): self.colorRegion(res, region) if self.getRegionArea(region) &gt; self.grayImg.shape[0] * self.grayImg.shape[1] * AREA_LIM: #number of regions meeting the area criteria n2 += 1 self.colorRegion(res2, region) if self.getRegionPerimeter(region) &gt; 2 * (self.grayImg.shape[0] + \\ self.grayImg.shape[1]) * PERIMETER_LIM: #number of regions meeting the perimeter criteria n3 += 1 self.colorRegion(res3, region) if self.getAspectRatio(region) &lt; ASPECT_RATIO_LIM: #number of regions meeting the aspect ratio criteria n4 += 1 self.colorRegion(res4, region) if (self.getOccupyRate(region) &gt; OCCUPATION_LIM[0]) and \\ (self.getOccupyRate(region) &lt; OCCUPATION_LIM[1]): n5 += 1 self.colorRegion(res5, region) if (self.getCompactness(region) &gt; \\COMPACTNESS_LIM[0]) and \\(self.getCompactness(region) &lt; \\COMPACTNESS_LIM[1]): #final number of regions left n6 += 1 self.colorRegion(res6, region) bboxes.append(mser_bboxes[i]) After eliminating non-text regions, I draw bounding boxes on the remaining regions andvoila, we have successfully detected and segmented the characters on the number plate.Note: Apply NMS to remove overlapping bounding boxes. 12for bbox in bboxes: cv2.rectangle(img,(bbox[0]-1,bbox[1]-1),(bbox[0]+bbox[2]+1,box[1]+bbox[3]+1),(255,0,0), 1) Enough coding. Let’s see some results. 1234567891011pltShow(&quot;MSER Result Analysis&quot;, \\ (self.img, &quot;Image&quot;), \\ (self.cannyImg, &quot;Canny&quot;), \\ (res, &quot;MSER,(&#123;&#125; regions)&quot;.format(n1)), \\ (res2, &quot;Area=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_areaLimit, n2)), \\ (res3, &quot;Perimeter=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_perimeterLimit, n3)), \\ (res4, &quot;Aspect Ratio=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_aspectRatioLimit, n4)), \\ (res5, &quot;Occupation=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_occupationLimit, n5)), \\ (res6, &quot;Compactness=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_compactnessLimit, n6)), \\ (boxRes, &quot;Segmented Image&quot;) \\ ) ConclusionIn this post, we covered the various image preprocessing techniques and learned about howto perform text localization on number plates.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/blog/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/blog/tags/Deep-Learning/"}],"author":"Akshay Bajpai"},{"title":"Detecting Lanes using Deep Neural Networks","slug":"Detecting-Lanes-using-Deep-Neural-Networks","date":"2019-11-26T11:17:10.000Z","updated":"2019-12-03T06:28:17.969Z","comments":true,"path":"/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/","link":"","permalink":"https://nayan.co/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/","excerpt":"","text":"This post explains how to use deep neural networks to detect highway lanes. Lane markings are the main static component on highways.They instruct the vehicles to interactively and safely drive on the highways. Lane detection is also an important task in autonomous driving, which provides localization information to the control of the car. It is also used in ADAS(Advanced Driver Assistance System). For the task of lane detection, we have two open-source datasets available. One is the Tusimple dataset and the other is the CULane dataset. Let’s have a brief look at one of the datasets. Tusimple DatasetThis dataset was released as part of the Tusimple Lane Detection Challenge. It contains 3626 video clips of 1 sec duration each. Each of these video clips contains 20 frames of which, the last frame is annotated. These videos were captured by mounting the cameras on a vehicle dashboard. You can download the dataset from here. The directory structure looks like the figure below, Each sub-directory contains 20 sequential images of which, the last frame is annotated. label_data_(date).json contains labels in JSON format for the last frame. Each line in the JSON file is a dictionary with key values… raw_file: string type. the file path in the clip lanes: it is a list of list of lanes. Each list corresponds to a lane and each element of the inner list is x-coordinate of ground truth lane. h_samples: it is a list of height values corresponding to the lanes. Each element in this list is y-coordinate of ground truth lane In this dataset, at most four lanes are annotated - the two ego lanes (two lane boundaries in which the vehicle is currently located) and the lanes to the right and left of ego lanes. All the lanes are annotated at an equal interval of height, therefore h_samples contain only one list whose elements correspond to y-coordinates for all lists in lanes. For a point in h_samples, if there is no lane at the location, its corresponding x-coordinate has -2. For example, a line in the JSON file looks like : 12345678910&#123; &quot;lanes&quot;: [ [-2, -2, -2, -2, 632, 625, 617, 609, 601, 594, 586, 578, 570, 563, 555, 547, 539, 532, 524, 516, 508, 501, 493, 485, 477, 469, 462, 454, 446, 438, 431, 423, 415, 407, 400, 392, 384, 376, 369, 361, 353, 345, 338, 330, 322, 314, 307, 299], [-2, -2, -2, -2, 719, 734, 748, 762, 777, 791, 805, 820, 834, 848, 863, 877, 891, 906, 920, 934, 949, 963, 978, 992, 1006, 1021, 1035, 1049, 1064, 1078, 1092, 1107, 1121, 1135, 1150, 1164, 1178, 1193, 1207, 1221, 1236, 1250, 1265, -2, -2, -2, -2, -2], [-2, -2, -2, -2, -2, 532, 503, 474, 445, 416, 387, 358, 329, 300, 271, 241, 212, 183, 154, 125, 96, 67, 38, 9, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2], [-2, -2, -2, 781, 822, 862, 903, 944, 984, 1025, 1066, 1107, 1147, 1188, 1229, 1269, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2] ], &quot;h_samples&quot;: [240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710], &quot;raw_file&quot;: &quot;path_to_clip&quot;&#125; It says that there are four lanes in the image, and the first lane starts at (632,280), the second lane starts at (719,280), the third lane starts at (532,290) and the fourth lane starts at (781,270). DataSet Visualization12345678910111213141516# import required packagesimport jsonimport numpy as npimport cv2import matplotlib.pyplot as plt# read each line of json filejson_gt = [json.loads(line) for line in open(&apos;label_data.json&apos;)]gt = json_gt[0]gt_lanes = gt[&apos;lanes&apos;]y_samples = gt[&apos;h_samples&apos;]raw_file = gt[&apos;raw_file&apos;]# see the imageimg = cv2.imread(raw_file)cv2.imshow(&apos;image&apos;,img)cv2.WaitKey(0)cv2.destroyAllWindows() Now see the JSON points visualization on the image 1234567gt_lanes_vis = [[(x, y) for (x, y) in zip(lane, y_samples) if x &gt;= 0] for lane in gt_lanes]img_vis = img.copy()for lane in gt_lanes_vis: cv2.polylines(img_vis, np.int32([lane]), isClosed=False, color=(0,255,0), thickness=5) Now, we have understood the dataset, but we can not pass the above image as a label for the neural network since grayscale images with values ranging from zero to num_classes -1 are to be passed to the deep convolution neural network to outputs an image containing predicted lanes. So, we need to generate label images for the JSON files. Label images can be generated using OpenCV by drawing lines passing through the points in the JSON file. OpenCV has an inbuilt function to draw multiple lines through a set of points. OpenCV’s Polylines method can be used here. First, create a mask of all zeros with height and width equal to the raw file’s height and width using numpy. The image size can be reduced to maintain lesser computations during training, but do not forget to maintain the same aspect ratio. Generating LabelsThe label should be a grayscale image. Generate one label for each clip from the JSON file. First, create a mask of black pixels with a shape similar to the raw_file image from the JSON file. Now, using OpenCV’s polylines method draw lines with different colors (each corresponding to each lane in lanes) on the mask image using the lanes and h_samples from the JSON file. From the three channeled mask image generate a gray scale mask image with values as class numbers. Likewise, create labels for all the images in the JSON file. You can resize the image and its label to a smaller size for lesser computations. 12345678mask = np.zeros_like(img)colors = [[255,0,0],[0,255,0],[0,0,255],[0,255,255]]for i in range(len(gt_lanes_vis)): cv2.polylines(mask, np.int32([gt_lanes_vis[i]]), isClosed=False,color=colors[i], thickness=5)!! create grey-scale label imagelabel = np.zeros((720,1280),dtype = np.uint8)for i in range(len(colors)): label[np.where((mask == colors[i]).all(axis = 2))] = i+1 Build and Train ModelLane Detection is essentially an image segmentation problem. So I am using the ERFNET model for this task, which is efficient and fast. Originally ERFNET was proposed for semantic segmentation problems, but it can also be extended to other image segmentation problems. You can check out for its paper here. It is a CNN with Encoder, Decoder and dilated convolutions along with non-bottleneck residual layers. See Fig.1 for model architecture. Build and create an object of the model. Train it over the dataset created above, for a sufficient number of epochs with Binary Cross Entropy loss or custom loss function which minimizes the per-pixel error. For better memory usage, create a dataset generator and train the model over it. Generators remove the burden of loading all the images into memory (if your dataset is of large size, you should use a generator) which leads to eating up of all memory and the other processes can’t work properly. Fig 2 shows the layers of ERFNET with input and output dimensions. Evaluate ModelAfter training, get the model’s predictions using the code snippet below. I have implemented this in Pytorch. I use the color_lanes method to convert output images from the model (which are two channeled with values as class numbers) to three channeled images. im_seg is the final overlayed image shown in Image 4. 123456789101112131415161718192021222324252627282930313233343536#using pytorchimport torchfrom torchvision.transforms import ToTensordef color_lanes(image, classes, i, color, HEIGHT, WIDTH): buffer_c1 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8) buffer_c1[classes == i] = color[0] image[:, :, 0] += buffer_c1 buffer_c2 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8) buffer_c2[classes == i] = color[1] image[:, :, 1] += buffer_c2 buffer_c3 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8) buffer_c3[classes == i] = color[2] image[:, :, 2] += buffer_c3 return imageimg = cv2.imread(&apos;images/test.jpg&apos;) img = cv2.resize(img,(WIDTH, HEIGHT),interpolation = cv2.INETR_CUBIC)op_transforms = transforms.Compose([transforms.ToTensor()])device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)im_tensor = torch.unsqueeze(op_transforms(img), dim=0)im_tensor = im_tensor.to(device)model = ERFNET(5)model = model.to(device)model = model.eval()out = model(im_tensor)out = out.max(dim=1)[1]out_np = out.cpu().numpy()[0]out_viz = np.zeros((HEIGHT, WIDTH, 3))for i in range(1, NUM_LD_CLASSES): rand_c1 = random.randint(1, 255) rand_c2 = random.randint(1, 255) rand_c3 = random.randint(1, 255) out_viz = color_lanes( out_viz, out_np, i, (rand_c1, rand_c2, rand_c3), HEIGHT, WIDTH)instance_im = out_viz.astype(np.uint8)im_seg = cv2.addWeighted(img, 1, instance_im, 1, 0) Thanks for reading it… References ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic 2. Segmentation. Lane Detection and Classification using CNNs. https://www.mdpi.com/sensors/sensors-19-00503/article_deploy/html/images/sensors-19-00503-g004.png","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/blog/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/blog/tags/Deep-Learning/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/blog/tags/Machine-Learning/"},{"name":"Lane Detection","slug":"Lane-Detection","permalink":"https://nayan.co/blog/tags/Lane-Detection/"},{"name":"Advance Driver Assistance","slug":"Advance-Driver-Assistance","permalink":"https://nayan.co/blog/tags/Advance-Driver-Assistance/"},{"name":"Autonomous Driving","slug":"Autonomous-Driving","permalink":"https://nayan.co/blog/tags/Autonomous-Driving/"}],"author":"Anand Kummari"}]}